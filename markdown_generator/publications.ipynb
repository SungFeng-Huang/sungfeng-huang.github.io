{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Publications markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `publications.py`. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one containing your data.\n",
    "\n",
    "TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n",
    "\n",
    "- `excerpt` and `paper_url` can be blank, but the others must have values. \n",
    "- `pub_date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub_date\ttitle\tauthors\tvenue\texcerpt\tabstract\tcitation\turl_slug\tpaper_url\n",
      "2018-12-18\tPhonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval\tYi-Chen Chen, <u>Sung-Feng Huang</u>, Chia-Hao Shen, Hung-yi Lee, Lin-shan Lee\tIEEE Spoken Language Technology Workshop (SLT)\tPart of Audio Word2Vec Project.\tWord embedding or Word2Vec has been successful in offering semantics for text words learned from the context of words. Audio Word2Vec was shown to offer phonetic structures for spoken words (signal segments for words) learned from signals within spoken words. This paper proposes a two-stage framework to perform phonetic-and-semantic embedding on spoken words considering the context of the spoken words. Stage 1 performs phonetic embedding with speaker characteristics disentangled. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings. In general, phonetic structure and semantics inevitably disturb each other. For example the words “brother” and “sister” are close in semantics but very different in phonetic structure, while the words “brother” and “bother” are in the other way around. But phonetic-and-semantic embedding is attractive, as shown in the initial experiments on spoken document retrieval. Not only spoken documents including the spoken query can be retrieved based on the phonetic structures, but spoken documents semantically related to the query but not including the query can also be retrieved based on the semantics.\tYi-Chen Chen, Sung-Feng Huang, Chia-Hao Shen, Hung-yi Lee and Lin-shan Lee, \"Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval,\" 2018 IEEE Spoken Language Technology Workshop (SLT), Athens, Greece, 2018, pp. 941-948, doi: 10.1109/SLT.2018.8639553.\tchen2018phonetic\thttps://ieeexplore.ieee.org/abstract/document/8639553\n",
      "2019-06-13\tAudio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation\tYi-Chen Chen, <u>Sung-Feng Huang</u>, Hung-yi Lee, Yu-Hsuan Wang, Chia-Hao Shen\tIEEE/ACM TASLP\tPart of Audio Word2Vec Project.\tIn text, word2vec transforms each word into a fixed-size vector used as the basic component in applications of natural language processing. Given a large collection of unannotated audio, audio word2vec can also be trained in an unsupervised way using a sequence-to-sequence autoencoder (SA). These vector representations are shown to effectively describe the sequential phonetic structures of the audio segments. In this paper, we further extend this research in the following two directions. First, we disentangle phonetic information and speaker information from the SA vector representations. Second, we extend audio word2vec from the word level to the utterance level by proposing a new segmental audio word2vec in which unsupervised spoken word boundary segmentation and audio word2vec are jointly learned and mutually enhanced, and utterances are directly represented as sequences of vectors carrying phonetic information. This is achieved by means of a segmental sequence-to-sequence autoencoder, in which a segmentation gate trained with reinforcement learning is inserted in the encoder.\tYi-Chen Chen, Sung-Feng Huang, Hung-yi Lee, Yu-Hsuan Wang and Chia-Hao Shen, \"Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 27.9 (2019): 1481-1493.\tchen2018audio\thttps://ieeexplore.ieee.org/abstract/document/8736337\n",
      "2020-10-06\tPretrained Language Model Embryology: The Birth of ALBERT\tCheng-Han Chiang, <u>Sung-Feng Huang</u>, Hung-yi Lee\tEMNLP\tThe results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining, and it is found that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance.\tWhile behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.\tChiang, C., Huang, S., & Lee, H. (2020). Pretrained Language Model Embryology: The Birth of ALBERT. ArXiv, abs/2010.02480. \tchiang2020pretrained\thttps://arxiv.org/abs/2010.02480\n",
      "2020-10-29\tStabilizing Label Assignment for Speech Separation by Self-supervised Pre-training \t<u>Sung-Feng Huang</u>, Shun-Po Chuang, Da-Rong Liu, Yi-Chen Chen, Gene-Ping Yang, Hung-yi Lee\tInterspeech\tSSL for speech separation. \tSpeech separation has been well developed, with the very successful permutation invariant training (PIT) approach, although the frequent label assignment switching happening during PIT training remains to be a problem when better convergence speed and achievable performance are desired. In this paper, we propose to perform self-supervised pre-training to stabilize the label assignment in training the speech separation model. Experiments over several types of self-supervised approaches, several typical speech separation models and two different datasets showed that very good improvements are achievable if a proper self-supervised approach is chosen.\tHuang, S.-F., Chuang, S.-P., Liu, D.-R., Chen, Y.-C., Yang, G.-P., Lee, H.-y. (2021) Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training. Proc. Interspeech 2021, 3056-3060, doi: 10.21437/Interspeech.2021-763 \thuang2020stabilizing\thttps://www.isca-archive.org/interspeech_2021/huang21h_interspeech.html\n",
      "2021-12-13\tNon-Autoregressive Mandarin-English Code-Switching Speech Recognition\tShun-Po Chuang, Heng-Jui Chang, <u>Sung-Feng Huang</u>, Hung-yi Lee\tIEEE ASRU\tMask-CTC NAR ASR framework to tackle the CS speech recognition issue.\tMandarin-English code-switching (CS) is frequently used among East and Southeast Asian people. However, the intra-sentence language switching of the two very different languages makes recognizing CS speech challenging. Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the need for left-to-right beam decoding in autoregressive (AR) models and achieved outstanding performance and fast inference speed, but it has not been applied to Mandarin-English CS speech recognition. This paper takes advantage of the Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We further propose to change the Mandarin output target of the encoder to Pinyin for faster encoder training and introduce the Pinyin-to-Mandarin decoder to learn contextualized information. Moreover, we use word embedding label smoothing to regularize the decoder with contextualized information and projection matrix regularization to bridge that gap between the encoder and decoder. We evaluate these methods on the SEAME corpus and achieved exciting results.\tChuang, Shun-Po, Heng-Jui Chang, Sung-Feng Huang, and Hung-yi Lee. \"Non-autoregressive mandarin-english code-switching speech recognition.\" In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 465-472. IEEE, 2021.\tchuang2021non\thttps://ieeexplore.ieee.org/abstract/document/9688174\n",
      "2021-12-18\t\"Learning Phone Recognition From Unpaired Audio and Phone Sequences Based on Generative Adversarial Network\n",
      "\"\tDa-rong Liu, Po-chun Hsu, Yi-chen Chen, <u>Sung-feng Huang</u>, Shun-po Chuang, Da-yi Wu, Hung-yi Lee\tIEEE/ACM TASLP\tUnsupervised ASR\tASR has been shown to achieve great performance recently. However, most of them rely on massive paired data, which is not feasible for low-resource languages worldwide. This paper investigates how to learn directly from unpaired phone sequences and speech utterances. We design a two-stage iterative framework. GAN training is adopted in the first stage to find the mapping relationship between unpaired speech and phone sequence. In the second stage, another HMM model is introduced to train from the generator’s output, which boosts the performance and provides a better segmentation for the next iteration. In the experiment, we first investigate different choices of model designs. Thenwe compare the framework to different types of baselines: (i) supervised methods (ii) acoustic unit discovery based methods (iii) methods learning from unpaired data. Our framework performs consistently better than all acoustic unit discovery methods and previous methods learning from unpaired data based on the TIMIT dataset.\tLiu, Da-rong, Po-chun Hsu, Yi-chen Chen, Sung-feng Huang, Shun-po Chuang, Da-yi Wu, and Hung-yi Lee. \"Learning phone recognition from unpaired audio and phone sequences based on generative adversarial network.\" IEEE/ACM transactions on audio, speech, and language processing 30 (2021): 230-243.\tliu2021learning\thttps://ieeexplore.ieee.org/abstract/document/9664381/\n",
      "2022-04-13\t\"Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech\n",
      "\"\t<u>Sung-Feng Huang</u>, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, Hung-yi Lee\tIEEE/ACM TASLP\tMeta-learning for few-shot speaker adaptive text-to-speech\tPersonalizing a speech synthesis system is a highly desired application, where the system can generate speech with the user’s voice with rare enrolled recordings. There are two main approaches to build such a system in recent works: speaker adaptation and speaker encoding. On the one hand, speaker adaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model with few enrolled samples. However, they require at least thousands of fine-tuning steps for high-quality adaptation, making it hard to apply on devices. On the other hand, speaker encoding methods encode enrollment utterances into a speaker embedding. The trained TTS model can synthesize the user’s speech conditioned on the corresponding speaker embedding. Nevertheless, the speaker encoder suffers from the generalization gap between the seen and unseen speakers. In this paper, we propose applying a meta-learning algorithm to the speaker adaptation method. More specifically, we use Model Agnostic Meta-Learning (MAML) as the training algorithm of a multi-speaker TTS model, which aims to find a great meta-initialization to adapt the model to any few-shot speaker adaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS model to unseen speakers efficiently. Our experiments compare the proposed method (Meta-TTS) with two baselines: a speaker adaptation method baseline and a speaker encoding method baseline. The evaluation results show that Meta-TTS can synthesize high speaker-similarity speech from few enrollment samples with fewer adaptation steps than the speaker adaptation baseline and outperforms the speaker encoding baseline under the same training scheme. When the speaker encoder of the baseline is pre-trained with extra 8371 speakers of data, Meta-TTS can still outperform the baseline on LibriTTS dataset and achieve comparable results on VCTK dataset.\tHuang, Sung-Feng, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, and Hung-yi Lee. \"Meta-tts: Meta-learning for few-shot speaker adaptive text-to-speech.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2022): 1558-1571.\thuang2022meta\thttps://ieeexplore.ieee.org/abstract/document/9756900\n",
      "2023-06-04\tPersonalized Lightweight Text-to-Speech: Voice Cloning with Adaptive Structured Pruning\t\"<u>Sung-Feng Huang</u>, Chia-Ping Chen, Zhi-Sheng Chen, Yu-Pao Tsai, Hung-yi Lee\n",
      "\"\tIEEE ICASSP\tLearnable model pruning for TTS fine-tuning\tPersonalized TTS is an exciting and highly desired application that allows users to train their TTS voice using only a few recordings. However, TTS training typically requires many hours of recording and a large model, making it unsuitable for deployment on mobile devices. To overcome this limitation, related works typically require fine-tuning a pre-trained TTS model to preserve its ability to generate high-quality audio samples while adapting to the target speaker’s voice. This process is commonly referred to as \"voice cloning.\" Although related works have achieved significant success in changing the TTS model’s voice, they are still required to fine-tune from a large pre-trained model, resulting in a significant size for the voice-cloned model. In this paper, we propose applying trainable structured pruning to voice cloning. By training the structured pruning masks with voice-cloning data, we can produce a unique pruned model for each target speaker. Our experiments demonstrate that using learnable structured pruning, we can compress the model size to 7 times smaller while achieving comparable voice-cloning performance.\tHuang, Sung-Feng, Chia-Ping Chen, Zhi-Sheng Chen, Yu-Pao Tsai, and Hung-yi Lee. \"Personalized Lightweight Text-to-Speech: Voice Cloning with Adaptive Structured Pruning.\" In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023.\thuang2023personalized\thttps://ieeexplore.ieee.org/abstract/document/10097178\n",
      "2023-12-16\tMaximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization\tWei-Ping Huang, <u>Sung-Feng Huang</u>, Hung-yi Lee\tIEEE ASRU\tUtilize unlabeled speech data for few-shot cross-lingual TTS adaptation\tThis paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.\tHuang, Wei-Ping, Sung-Feng Huang, and Hung-yi Lee. \"Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization.\" In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 1-8. IEEE, 2023.\thuang2023maximizing\thttps://ieeexplore.ieee.org/abstract/document/10389665\n",
      "2025-04-26\tGenerative Speech Foundation Model Pretraining for High-Quality Speech Extraction and Restoration\tPin-Jui Ku, Alexander H. Liu, Roman Korostik, <u>Sung-Feng Huang</u>, Szu-Wei Fu, Ante Jukić\tIEEE ICASSP\tFoundation flow-matching model for generation tasks\tThis paper proposes a generative pretraining foundation model for high-quality speech restoration tasks. By directly operating on complex-valued short-time Fourier transform coefficients, our model does not rely on any vocoders for time-domain signal reconstruction. As a result, our model simplifies the synthesis process and removes the quality upper-bound introduced by any mel-spectrogram vocoder compared to prior work SpeechFlow. The proposed method is evaluated on multiple speech restoration tasks, including speech denoising, bandwidth extension, codec artifact removal, and target speaker extraction. In all scenarios, finetuning our pretrained model results in superior performance over strong baselines. Notably, in the target speaker extraction task, our model outperforms existing systems, including those leveraging SSL-pretrained encoders like WavLM. The code and the pretrained checkpoints are publicly available in the NVIDIA NeMo framework.\tKu, P. J., Liu, A. H., Korostik, R., Huang, S. F., Fu, S. W., & Jukić, A. (2024). Generative speech foundation model pretraining for high-quality speech extraction and restoration. arXiv preprint arXiv:2409.16117.\tku2024generative\thttps://arxiv.org/abs/2409.16117\n",
      "2024-12-02\tDetecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits\t\"<u>Sung-Feng Huang</u>, Heng-Cheng Kuo, Zhehuai Chen, Xuesong Yang, Chao-Han Huck Yang, Yu Tsao, Yu-Chiang Frank Wang, Hung-yi Lee, Szu-Wei Fu\n",
      "\"\tIEEE SLT\tSpeech editing dataset & edit deepfake detection\tNeural speech editing advancements have raised concerns about their misuse in spoofing attacks. Traditional partially edited speech corpora primarily focus on cut-and-paste edits, which, while maintaining speaker consistency, often introduce detectable discontinuities. Recent methods, like A3 T and Voicebox, improve transitions by leveraging contextual information. To foster spoofing detection research, we introduce the Speech INfilling Edit (SINE) dataset, created with Voicebox. We detailed the process of re-implementing Voicebox training and dataset creation. Subjective evaluations confirm that speech edited using this novel technique is more challenging to detect than conventional cut-and-paste methods. Despite human difficulty, experimental results demonstrate that self-supervised-based detectors can achieve remarkable performance in detection, localization, and generalization across different edit methods. The dataset and related models will be made available at: https://jasonswfu.github.io/SINE_dataset/index.html\tHuang, Sung-Feng, Heng-Cheng Kuo, Zhehuai Chen, Xuesong Yang, Chao-Han Huck Yang, Yu Tsao, Yu-Chiang Frank Wang, Hung-yi Lee, and Szu-Wei Fu. \"Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits.\" In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 652-659. IEEE, 2024.\thuang2024detecting\thttps://ieeexplore.ieee.org/abstract/document/10832200/\n"
     ]
    }
   ],
   "source": [
    "!cat publications.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "\n",
    "We are using the very handy pandas library for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_date</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>abstract</th>\n",
       "      <th>citation</th>\n",
       "      <th>url_slug</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>Phonetic-and-Semantic Embedding of Spoken word...</td>\n",
       "      <td>Yi-Chen Chen, &lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Chia-Hao...</td>\n",
       "      <td>IEEE SLT</td>\n",
       "      <td>Part of Audio Word2Vec Project.</td>\n",
       "      <td>Word embedding or Word2Vec has been successful...</td>\n",
       "      <td>Yi-Chen Chen, Sung-Feng Huang, Chia-Hao Shen, ...</td>\n",
       "      <td>chen2018phonetic</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>Audio Word2vec: Sequence-to-Sequence Autoencod...</td>\n",
       "      <td>Yi-Chen Chen, &lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Hung-yi ...</td>\n",
       "      <td>IEEE/ACM TASLP</td>\n",
       "      <td>Part of Audio Word2Vec Project.</td>\n",
       "      <td>In text, word2vec transforms each word into a ...</td>\n",
       "      <td>Yi-Chen Chen, Sung-Feng Huang, Hung-yi Lee, Yu...</td>\n",
       "      <td>chen2018audio</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>Pretrained Language Model Embryology: The Birt...</td>\n",
       "      <td>Cheng-Han Chiang, &lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Hung...</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>The results show that ALBERT learns to reconst...</td>\n",
       "      <td>While behaviors of pretrained language models ...</td>\n",
       "      <td>Chiang, C., Huang, S., &amp; Lee, H. (2020). Pretr...</td>\n",
       "      <td>chiang2020pretrained</td>\n",
       "      <td>https://arxiv.org/abs/2010.02480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>Stabilizing Label Assignment for Speech Separa...</td>\n",
       "      <td>&lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Shun-Po Chuang, Da-Ron...</td>\n",
       "      <td>Interspeech</td>\n",
       "      <td>SSL for speech separation.</td>\n",
       "      <td>Speech separation has been well developed, wit...</td>\n",
       "      <td>Huang, S.-F., Chuang, S.-P., Liu, D.-R., Chen,...</td>\n",
       "      <td>huang2020stabilizing</td>\n",
       "      <td>https://www.isca-archive.org/interspeech_2021/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>Non-Autoregressive Mandarin-English Code-Switc...</td>\n",
       "      <td>Shun-Po Chuang, Heng-Jui Chang, &lt;u&gt;Sung-Feng H...</td>\n",
       "      <td>IEEE ASRU</td>\n",
       "      <td>Mask-CTC NAR ASR framework to tackle the CS sp...</td>\n",
       "      <td>Mandarin-English code-switching (CS) is freque...</td>\n",
       "      <td>Chuang, Shun-Po, Heng-Jui Chang, Sung-Feng Hua...</td>\n",
       "      <td>chuang2021non</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>Learning Phone Recognition From Unpaired Audio...</td>\n",
       "      <td>Da-rong Liu, Po-chun Hsu, Yi-chen Chen, &lt;u&gt;Sun...</td>\n",
       "      <td>IEEE/ACM TASLP</td>\n",
       "      <td>Unsupervised ASR</td>\n",
       "      <td>ASR has been shown to achieve great performanc...</td>\n",
       "      <td>Liu, Da-rong, Po-chun Hsu, Yi-chen Chen, Sung-...</td>\n",
       "      <td>liu2021learning</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>Meta-TTS: Meta-Learning for Few-Shot Speaker A...</td>\n",
       "      <td>&lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Chyi-Jiunn Lin, Da-Ron...</td>\n",
       "      <td>IEEE/ACM TASLP</td>\n",
       "      <td>Meta-learning for few-shot speaker adaptive te...</td>\n",
       "      <td>Personalizing a speech synthesis system is a h...</td>\n",
       "      <td>Huang, Sung-Feng, Chyi-Jiunn Lin, Da-Rong Liu,...</td>\n",
       "      <td>huang2022meta</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-04</td>\n",
       "      <td>Personalized Lightweight Text-to-Speech: Voice...</td>\n",
       "      <td>&lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Chia-Ping Chen, Zhi-Sh...</td>\n",
       "      <td>IEEE ICASSP</td>\n",
       "      <td>Learnable model pruning for TTS fine-tuning</td>\n",
       "      <td>Personalized TTS is an exciting and highly des...</td>\n",
       "      <td>Huang, Sung-Feng, Chia-Ping Chen, Zhi-Sheng Ch...</td>\n",
       "      <td>huang2023personalized</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>Maximizing Data Efficiency for Cross-Lingual T...</td>\n",
       "      <td>Wei-Ping Huang, &lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Hung-y...</td>\n",
       "      <td>IEEE ASRU</td>\n",
       "      <td>Utilize unlabeled speech data for few-shot cro...</td>\n",
       "      <td>This paper presents an effective transfer lear...</td>\n",
       "      <td>Huang, Wei-Ping, Sung-Feng Huang, and Hung-yi ...</td>\n",
       "      <td>huang2023maximizing</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>Generative Speech Foundation Model Pretraining...</td>\n",
       "      <td>Pin-Jui Ku, Alexander H. Liu, Roman Korostik, ...</td>\n",
       "      <td>IEEE ICASSP</td>\n",
       "      <td>Foundation flow-matching model for generation ...</td>\n",
       "      <td>This paper proposes a generative pretraining f...</td>\n",
       "      <td>Ku, P. J., Liu, A. H., Korostik, R., Huang, S....</td>\n",
       "      <td>ku2024generative</td>\n",
       "      <td>https://arxiv.org/abs/2409.16117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>Detecting the Undetectable: Assessing the Effi...</td>\n",
       "      <td>&lt;u&gt;Sung-Feng Huang&lt;/u&gt;, Heng-Cheng Kuo, Zhehua...</td>\n",
       "      <td>IEEE SLT</td>\n",
       "      <td>Speech editing dataset &amp; edit deepfake detection</td>\n",
       "      <td>Neural speech editing advancements have raised...</td>\n",
       "      <td>Huang, Sung-Feng, Heng-Cheng Kuo, Zhehuai Chen...</td>\n",
       "      <td>huang2024detecting</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_date                                              title  \\\n",
       "0   2018-12-18  Phonetic-and-Semantic Embedding of Spoken word...   \n",
       "1   2019-06-13  Audio Word2vec: Sequence-to-Sequence Autoencod...   \n",
       "2   2020-10-06  Pretrained Language Model Embryology: The Birt...   \n",
       "3   2020-10-29  Stabilizing Label Assignment for Speech Separa...   \n",
       "4   2021-12-13  Non-Autoregressive Mandarin-English Code-Switc...   \n",
       "5   2021-12-18  Learning Phone Recognition From Unpaired Audio...   \n",
       "6   2022-04-13  Meta-TTS: Meta-Learning for Few-Shot Speaker A...   \n",
       "7   2023-06-04  Personalized Lightweight Text-to-Speech: Voice...   \n",
       "8   2023-12-16  Maximizing Data Efficiency for Cross-Lingual T...   \n",
       "9   2025-04-26  Generative Speech Foundation Model Pretraining...   \n",
       "10  2024-12-02  Detecting the Undetectable: Assessing the Effi...   \n",
       "\n",
       "                                              authors           venue  \\\n",
       "0   Yi-Chen Chen, <u>Sung-Feng Huang</u>, Chia-Hao...        IEEE SLT   \n",
       "1   Yi-Chen Chen, <u>Sung-Feng Huang</u>, Hung-yi ...  IEEE/ACM TASLP   \n",
       "2   Cheng-Han Chiang, <u>Sung-Feng Huang</u>, Hung...           EMNLP   \n",
       "3   <u>Sung-Feng Huang</u>, Shun-Po Chuang, Da-Ron...     Interspeech   \n",
       "4   Shun-Po Chuang, Heng-Jui Chang, <u>Sung-Feng H...       IEEE ASRU   \n",
       "5   Da-rong Liu, Po-chun Hsu, Yi-chen Chen, <u>Sun...  IEEE/ACM TASLP   \n",
       "6   <u>Sung-Feng Huang</u>, Chyi-Jiunn Lin, Da-Ron...  IEEE/ACM TASLP   \n",
       "7   <u>Sung-Feng Huang</u>, Chia-Ping Chen, Zhi-Sh...     IEEE ICASSP   \n",
       "8   Wei-Ping Huang, <u>Sung-Feng Huang</u>, Hung-y...       IEEE ASRU   \n",
       "9   Pin-Jui Ku, Alexander H. Liu, Roman Korostik, ...     IEEE ICASSP   \n",
       "10  <u>Sung-Feng Huang</u>, Heng-Cheng Kuo, Zhehua...        IEEE SLT   \n",
       "\n",
       "                                              excerpt  \\\n",
       "0                     Part of Audio Word2Vec Project.   \n",
       "1                     Part of Audio Word2Vec Project.   \n",
       "2   The results show that ALBERT learns to reconst...   \n",
       "3                         SSL for speech separation.    \n",
       "4   Mask-CTC NAR ASR framework to tackle the CS sp...   \n",
       "5                                    Unsupervised ASR   \n",
       "6   Meta-learning for few-shot speaker adaptive te...   \n",
       "7         Learnable model pruning for TTS fine-tuning   \n",
       "8   Utilize unlabeled speech data for few-shot cro...   \n",
       "9   Foundation flow-matching model for generation ...   \n",
       "10   Speech editing dataset & edit deepfake detection   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Word embedding or Word2Vec has been successful...   \n",
       "1   In text, word2vec transforms each word into a ...   \n",
       "2   While behaviors of pretrained language models ...   \n",
       "3   Speech separation has been well developed, wit...   \n",
       "4   Mandarin-English code-switching (CS) is freque...   \n",
       "5   ASR has been shown to achieve great performanc...   \n",
       "6   Personalizing a speech synthesis system is a h...   \n",
       "7   Personalized TTS is an exciting and highly des...   \n",
       "8   This paper presents an effective transfer lear...   \n",
       "9   This paper proposes a generative pretraining f...   \n",
       "10  Neural speech editing advancements have raised...   \n",
       "\n",
       "                                             citation               url_slug  \\\n",
       "0   Yi-Chen Chen, Sung-Feng Huang, Chia-Hao Shen, ...       chen2018phonetic   \n",
       "1   Yi-Chen Chen, Sung-Feng Huang, Hung-yi Lee, Yu...          chen2018audio   \n",
       "2   Chiang, C., Huang, S., & Lee, H. (2020). Pretr...   chiang2020pretrained   \n",
       "3   Huang, S.-F., Chuang, S.-P., Liu, D.-R., Chen,...   huang2020stabilizing   \n",
       "4   Chuang, Shun-Po, Heng-Jui Chang, Sung-Feng Hua...          chuang2021non   \n",
       "5   Liu, Da-rong, Po-chun Hsu, Yi-chen Chen, Sung-...        liu2021learning   \n",
       "6   Huang, Sung-Feng, Chyi-Jiunn Lin, Da-Rong Liu,...          huang2022meta   \n",
       "7   Huang, Sung-Feng, Chia-Ping Chen, Zhi-Sheng Ch...  huang2023personalized   \n",
       "8   Huang, Wei-Ping, Sung-Feng Huang, and Hung-yi ...    huang2023maximizing   \n",
       "9   Ku, P. J., Liu, A. H., Korostik, R., Huang, S....       ku2024generative   \n",
       "10  Huang, Sung-Feng, Heng-Cheng Kuo, Zhehuai Chen...     huang2024detecting   \n",
       "\n",
       "                                            paper_url  \n",
       "0   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "1   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "2                    https://arxiv.org/abs/2010.02480  \n",
       "3   https://www.isca-archive.org/interspeech_2021/...  \n",
       "4   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "5   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "6   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "7   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "8   https://ieeexplore.ieee.org/abstract/document/...  \n",
       "9                    https://arxiv.org/abs/2409.16117  \n",
       "10  https://ieeexplore.ieee.org/abstract/document/...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\n",
    "publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for row, item in publications.iterrows():\n",
    "    \n",
    "    md_filename = str(item.pub_date) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.pub_date) + \"-\" + item.url_slug\n",
    "    year = item.pub_date[:4]\n",
    "    \n",
    "    ## YAML variables\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    \n",
    "    md += \"\"\"collection: publications\"\"\"\n",
    "    \n",
    "    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n",
    "\n",
    "    md += \"\\nauthors: '\" + item.authors + \"'\"\n",
    "    \n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\nexcerpt: '\" + html_escape(item.excerpt) + \"'\"\n",
    "    \n",
    "    md += \"\\ndate: \" + str(item.pub_date) \n",
    "    \n",
    "    md += \"\\nvenue: '\" + html_escape(item.venue) + \"'\"\n",
    "    \n",
    "    if len(str(item.paper_url)) > 5:\n",
    "        md += \"\\npaperurl: '\" + item.paper_url + \"'\"\n",
    "    \n",
    "    md += \"\\ncitation: '\" + html_escape(item.citation) + \"'\"\n",
    "    \n",
    "    md += \"\\n---\"\n",
    "\n",
    "    if len(str(item.abstract)) > 5:\n",
    "        md += \"\\n\\nAbstract:\\n---\\n\" + html_escape(item.abstract) + \"\\n\"\n",
    "    \n",
    "    ## Markdown description for individual page\n",
    "        \n",
    "    # if len(str(item.excerpt)) > 5:\n",
    "    #     md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n",
    "    \n",
    "    # if len(str(item.paper_url)) > 5:\n",
    "    #     md += \"\\n[Download paper here](\" + item.paper_url + \")\\n\" \n",
    "        \n",
    "    # md += \"\\nRecommended citation: \" + item.citation\n",
    "    \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "       \n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the publications directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-18-chen2018phonetic.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_publications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "title: \"Paper Title Number 1\"\r\n",
      "collection: publications\r\n",
      "permalink: /publication/2009-10-01-paper-title-number-1\r\n",
      "excerpt: 'This paper is about the number 1. The number 2 is left for future work.'\r\n",
      "date: 2009-10-01\r\n",
      "venue: 'Journal 1'\r\n",
      "paperurl: 'http://academicpages.github.io/files/paper1.pdf'\r\n",
      "citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'\r\n",
      "---\r\n",
      "This paper is about the number 1. The number 2 is left for future work.\r\n",
      "\r\n",
      "[Download paper here](http://academicpages.github.io/files/paper1.pdf)\r\n",
      "\r\n",
      "Recommended citation: Your Name, You. (2009). \"Paper Title Number 1.\" <i>Journal 1</i>. 1(1)."
     ]
    }
   ],
   "source": [
    "!cat ../_publications/2009-10-01-paper-title-number-1.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
