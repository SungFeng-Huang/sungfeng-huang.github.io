pub_date	title	authors	venue	excerpt	abstract	citation	url_slug	paper_url
2018-12-18	Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval	Yi-Chen Chen, <b>Sung-Feng Huang</b>, Chia-Hao Shen, Hung-yi Lee, Lin-shan Lee	IEEE SLT	Part of Audio Word2Vec Project.	Word embedding or Word2Vec has been successful in offering semantics for text words learned from the context of words. Audio Word2Vec was shown to offer phonetic structures for spoken words (signal segments for words) learned from signals within spoken words. This paper proposes a two-stage framework to perform phonetic-and-semantic embedding on spoken words considering the context of the spoken words. Stage 1 performs phonetic embedding with speaker characteristics disentangled. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings. In general, phonetic structure and semantics inevitably disturb each other. For example the words “brother” and “sister” are close in semantics but very different in phonetic structure, while the words “brother” and “bother” are in the other way around. But phonetic-and-semantic embedding is attractive, as shown in the initial experiments on spoken document retrieval. Not only spoken documents including the spoken query can be retrieved based on the phonetic structures, but spoken documents semantically related to the query but not including the query can also be retrieved based on the semantics.	Yi-Chen Chen, Sung-Feng Huang, Chia-Hao Shen, Hung-yi Lee and Lin-shan Lee, "Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval," 2018 IEEE Spoken Language Technology Workshop (SLT), Athens, Greece, 2018, pp. 941-948, doi: 10.1109/SLT.2018.8639553.	chen2018phonetic	https://ieeexplore.ieee.org/abstract/document/8639553
2019-06-13	Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation	Yi-Chen Chen, <b>Sung-Feng Huang</b>, Hung-yi Lee, Yu-Hsuan Wang, Chia-Hao Shen	IEEE/ACM TASLP	Part of Audio Word2Vec Project.	In text, word2vec transforms each word into a fixed-size vector used as the basic component in applications of natural language processing. Given a large collection of unannotated audio, audio word2vec can also be trained in an unsupervised way using a sequence-to-sequence autoencoder (SA). These vector representations are shown to effectively describe the sequential phonetic structures of the audio segments. In this paper, we further extend this research in the following two directions. First, we disentangle phonetic information and speaker information from the SA vector representations. Second, we extend audio word2vec from the word level to the utterance level by proposing a new segmental audio word2vec in which unsupervised spoken word boundary segmentation and audio word2vec are jointly learned and mutually enhanced, and utterances are directly represented as sequences of vectors carrying phonetic information. This is achieved by means of a segmental sequence-to-sequence autoencoder, in which a segmentation gate trained with reinforcement learning is inserted in the encoder.	Yi-Chen Chen, Sung-Feng Huang, Hung-yi Lee, Yu-Hsuan Wang and Chia-Hao Shen, "Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation," IEEE/ACM Transactions on Audio, Speech, and Language Processing 27.9 (2019): 1481-1493.	chen2018audio	https://ieeexplore.ieee.org/abstract/document/8736337
2020-10-06	Pretrained Language Model Embryology: The Birth of ALBERT	Cheng-Han Chiang, <b>Sung-Feng Huang</b>, Hung-yi Lee	EMNLP	The results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining, and it is found that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance.	While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.	Chiang, C., Huang, S., & Lee, H. (2020). Pretrained Language Model Embryology: The Birth of ALBERT. ArXiv, abs/2010.02480. 	chiang2020pretrained	https://arxiv.org/abs/2010.02480
2020-10-29	Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training 	<b>Sung-Feng Huang</b>, Shun-Po Chuang, Da-Rong Liu, Yi-Chen Chen, Gene-Ping Yang, Hung-yi Lee	Interspeech	SSL for speech separation. 	Speech separation has been well developed, with the very successful permutation invariant training (PIT) approach, although the frequent label assignment switching happening during PIT training remains to be a problem when better convergence speed and achievable performance are desired. In this paper, we propose to perform self-supervised pre-training to stabilize the label assignment in training the speech separation model. Experiments over several types of self-supervised approaches, several typical speech separation models and two different datasets showed that very good improvements are achievable if a proper self-supervised approach is chosen.	Huang, S.-F., Chuang, S.-P., Liu, D.-R., Chen, Y.-C., Yang, G.-P., Lee, H.-y. (2021) Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training. Proc. Interspeech 2021, 3056-3060, doi: 10.21437/Interspeech.2021-763 	huang2020stabilizing	https://www.isca-archive.org/interspeech_2021/huang21h_interspeech.html
2021-12-13	Non-Autoregressive Mandarin-English Code-Switching Speech Recognition	Shun-Po Chuang, Heng-Jui Chang, <b>Sung-Feng Huang</b>, Hung-yi Lee	IEEE ASRU	Mask-CTC NAR ASR framework to tackle the CS speech recognition issue.	Mandarin-English code-switching (CS) is frequently used among East and Southeast Asian people. However, the intra-sentence language switching of the two very different languages makes recognizing CS speech challenging. Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the need for left-to-right beam decoding in autoregressive (AR) models and achieved outstanding performance and fast inference speed, but it has not been applied to Mandarin-English CS speech recognition. This paper takes advantage of the Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We further propose to change the Mandarin output target of the encoder to Pinyin for faster encoder training and introduce the Pinyin-to-Mandarin decoder to learn contextualized information. Moreover, we use word embedding label smoothing to regularize the decoder with contextualized information and projection matrix regularization to bridge that gap between the encoder and decoder. We evaluate these methods on the SEAME corpus and achieved exciting results.	Chuang, Shun-Po, Heng-Jui Chang, Sung-Feng Huang, and Hung-yi Lee. "Non-autoregressive mandarin-english code-switching speech recognition." In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 465-472. IEEE, 2021.	chuang2021non	https://ieeexplore.ieee.org/abstract/document/9688174
2021-12-18	"Learning Phone Recognition From Unpaired Audio and Phone Sequences Based on Generative Adversarial Network
"	Da-rong Liu, Po-chun Hsu, Yi-chen Chen, <b>Sung-feng Huang</b>, Shun-po Chuang, Da-yi Wu, Hung-yi Lee	IEEE/ACM TASLP	Unsupervised ASR	ASR has been shown to achieve great performance recently. However, most of them rely on massive paired data, which is not feasible for low-resource languages worldwide. This paper investigates how to learn directly from unpaired phone sequences and speech utterances. We design a two-stage iterative framework. GAN training is adopted in the first stage to find the mapping relationship between unpaired speech and phone sequence. In the second stage, another HMM model is introduced to train from the generator’s output, which boosts the performance and provides a better segmentation for the next iteration. In the experiment, we first investigate different choices of model designs. Thenwe compare the framework to different types of baselines: (i) supervised methods (ii) acoustic unit discovery based methods (iii) methods learning from unpaired data. Our framework performs consistently better than all acoustic unit discovery methods and previous methods learning from unpaired data based on the TIMIT dataset.	Liu, Da-rong, Po-chun Hsu, Yi-chen Chen, Sung-feng Huang, Shun-po Chuang, Da-yi Wu, and Hung-yi Lee. "Learning phone recognition from unpaired audio and phone sequences based on generative adversarial network." IEEE/ACM transactions on audio, speech, and language processing 30 (2021): 230-243.	liu2021learning	https://ieeexplore.ieee.org/abstract/document/9664381/
2022-04-13	"Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech
"	<b>Sung-Feng Huang</b>, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, Hung-yi Lee	IEEE/ACM TASLP	Meta-learning for few-shot speaker adaptive text-to-speech	Personalizing a speech synthesis system is a highly desired application, where the system can generate speech with the user’s voice with rare enrolled recordings. There are two main approaches to build such a system in recent works: speaker adaptation and speaker encoding. On the one hand, speaker adaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model with few enrolled samples. However, they require at least thousands of fine-tuning steps for high-quality adaptation, making it hard to apply on devices. On the other hand, speaker encoding methods encode enrollment utterances into a speaker embedding. The trained TTS model can synthesize the user’s speech conditioned on the corresponding speaker embedding. Nevertheless, the speaker encoder suffers from the generalization gap between the seen and unseen speakers. In this paper, we propose applying a meta-learning algorithm to the speaker adaptation method. More specifically, we use Model Agnostic Meta-Learning (MAML) as the training algorithm of a multi-speaker TTS model, which aims to find a great meta-initialization to adapt the model to any few-shot speaker adaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS model to unseen speakers efficiently. Our experiments compare the proposed method (Meta-TTS) with two baselines: a speaker adaptation method baseline and a speaker encoding method baseline. The evaluation results show that Meta-TTS can synthesize high speaker-similarity speech from few enrollment samples with fewer adaptation steps than the speaker adaptation baseline and outperforms the speaker encoding baseline under the same training scheme. When the speaker encoder of the baseline is pre-trained with extra 8371 speakers of data, Meta-TTS can still outperform the baseline on LibriTTS dataset and achieve comparable results on VCTK dataset.	Huang, Sung-Feng, Chyi-Jiunn Lin, Da-Rong Liu, Yi-Chen Chen, and Hung-yi Lee. "Meta-tts: Meta-learning for few-shot speaker adaptive text-to-speech." IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2022): 1558-1571.	huang2022meta	https://ieeexplore.ieee.org/abstract/document/9756900
2023-06-04	Personalized Lightweight Text-to-Speech: Voice Cloning with Adaptive Structured Pruning	"<b>Sung-Feng Huang</b>, Chia-Ping Chen, Zhi-Sheng Chen, Yu-Pao Tsai, Hung-yi Lee
"	IEEE ICASSP	Learnable model pruning for TTS fine-tuning	Personalized TTS is an exciting and highly desired application that allows users to train their TTS voice using only a few recordings. However, TTS training typically requires many hours of recording and a large model, making it unsuitable for deployment on mobile devices. To overcome this limitation, related works typically require fine-tuning a pre-trained TTS model to preserve its ability to generate high-quality audio samples while adapting to the target speaker’s voice. This process is commonly referred to as "voice cloning." Although related works have achieved significant success in changing the TTS model’s voice, they are still required to fine-tune from a large pre-trained model, resulting in a significant size for the voice-cloned model. In this paper, we propose applying trainable structured pruning to voice cloning. By training the structured pruning masks with voice-cloning data, we can produce a unique pruned model for each target speaker. Our experiments demonstrate that using learnable structured pruning, we can compress the model size to 7 times smaller while achieving comparable voice-cloning performance.	Huang, Sung-Feng, Chia-Ping Chen, Zhi-Sheng Chen, Yu-Pao Tsai, and Hung-yi Lee. "Personalized Lightweight Text-to-Speech: Voice Cloning with Adaptive Structured Pruning." In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. IEEE, 2023.	huang2023personalized	https://ieeexplore.ieee.org/abstract/document/10097178
2023-12-16	Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization	Wei-Ping Huang, <b>Sung-Feng Huang</b>, Hung-yi Lee	IEEE ASRU	Utilize unlabeled speech data for few-shot cross-lingual TTS adaptation	This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.	Huang, Wei-Ping, Sung-Feng Huang, and Hung-yi Lee. "Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization." In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 1-8. IEEE, 2023.	huang2023maximizing	https://ieeexplore.ieee.org/abstract/document/10389665
2025-04-26	Generative Speech Foundation Model Pretraining for High-Quality Speech Extraction and Restoration	Pin-Jui Ku, Alexander H. Liu, Roman Korostik, <b>Sung-Feng Huang</b>, Szu-Wei Fu, Ante Jukić	IEEE ICASSP	Foundation flow-matching model for generation tasks	This paper proposes a generative pretraining foundation model for high-quality speech restoration tasks. By directly operating on complex-valued short-time Fourier transform coefficients, our model does not rely on any vocoders for time-domain signal reconstruction. As a result, our model simplifies the synthesis process and removes the quality upper-bound introduced by any mel-spectrogram vocoder compared to prior work SpeechFlow. The proposed method is evaluated on multiple speech restoration tasks, including speech denoising, bandwidth extension, codec artifact removal, and target speaker extraction. In all scenarios, finetuning our pretrained model results in superior performance over strong baselines. Notably, in the target speaker extraction task, our model outperforms existing systems, including those leveraging SSL-pretrained encoders like WavLM. The code and the pretrained checkpoints are publicly available in the NVIDIA NeMo framework.	Ku, P. J., Liu, A. H., Korostik, R., Huang, S. F., Fu, S. W., & Jukić, A. (2024). Generative speech foundation model pretraining for high-quality speech extraction and restoration. arXiv preprint arXiv:2409.16117.	ku2024generative	https://arxiv.org/abs/2409.16117
2024-12-02	Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits	"<b>Sung-Feng Huang</b>, Heng-Cheng Kuo, Zhehuai Chen, Xuesong Yang, Chao-Han Huck Yang, Yu Tsao, Yu-Chiang Frank Wang, Hung-yi Lee, Szu-Wei Fu
"	IEEE SLT	Speech editing dataset & edit deepfake detection	Neural speech editing advancements have raised concerns about their misuse in spoofing attacks. Traditional partially edited speech corpora primarily focus on cut-and-paste edits, which, while maintaining speaker consistency, often introduce detectable discontinuities. Recent methods, like A3 T and Voicebox, improve transitions by leveraging contextual information. To foster spoofing detection research, we introduce the Speech INfilling Edit (SINE) dataset, created with Voicebox. We detailed the process of re-implementing Voicebox training and dataset creation. Subjective evaluations confirm that speech edited using this novel technique is more challenging to detect than conventional cut-and-paste methods. Despite human difficulty, experimental results demonstrate that self-supervised-based detectors can achieve remarkable performance in detection, localization, and generalization across different edit methods. The dataset and related models will be made available at: https://jasonswfu.github.io/SINE_dataset/index.html	Huang, Sung-Feng, Heng-Cheng Kuo, Zhehuai Chen, Xuesong Yang, Chao-Han Huck Yang, Yu Tsao, Yu-Chiang Frank Wang, Hung-yi Lee, and Szu-Wei Fu. "Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits." In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 652-659. IEEE, 2024.	huang2024detecting	https://ieeexplore.ieee.org/abstract/document/10832200/
