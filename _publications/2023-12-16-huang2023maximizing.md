---
title: "Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization"
collection: publications
permalink: /publication/2023-12-16-huang2023maximizing
authors: 'Wei-Ping Huang, <u>Sung-Feng Huang</u>, Hung-yi Lee'
excerpt: 'Utilize unlabeled speech data for few-shot cross-lingual TTS adaptation'
date: 2023-12-16
venue: 'IEEE ASRU'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/10389665'
citation: 'Huang, Wei-Ping, Sung-Feng Huang, and Hung-yi Lee. &quot;Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization.&quot; In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 1-8. IEEE, 2023.'
---

Abstract:
---
This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.
